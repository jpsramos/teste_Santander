{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d619e57-efa8-4a25-b157-101fd8788f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|            ip|client_identd|user_id|           datetime|             request|status| size|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|      GET / HTTP/1.1|   403|  202|\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|GET /favicon.ico ...|   404|  209|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|      GET / HTTP/1.1|   200| 9157|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/lo...|   200|10469|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/r...|   200| 1014|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/9...|   200| 6206|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/t...|   200|15779|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/th...|   200| 4492|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/li...|   200|25960|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/s...|   200|  168|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5604|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200|10556|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 9925|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/c...|   200|  979|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/h...|   200| 3892|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5397|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/l...|   200| 2767|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5766|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/h...|   200|68831|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:37|GET /assets/img/d...|   200| 5766|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Tabela Delta 'access_log' salva com sucesso.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "#from pyspark.sql.functions import to_timestamp, col\n",
    "import re\n",
    "\n",
    "# Criar uma SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Web Server Access Log\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho para o arquivo de log\n",
    "log_file_path = \"/FileStore/tables/access_log-5.txt\"  # Substitua pelo caminho do seu arquivo\n",
    "\n",
    "# Ler o arquivo de log como RDD\n",
    "logs_rdd = spark.sparkContext.textFile(log_file_path)\n",
    "\n",
    "# Regex ajustado para o formato do log fornecido\n",
    "log_pattern = r'(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\d+)'\n",
    "\n",
    "# Função para processar cada linha do log\n",
    "def parse_log_line(line):\n",
    "    match = re.match(log_pattern, line)\n",
    "    if match:\n",
    "        return (\n",
    "            match.group(1),  # ip\n",
    "            match.group(2),  # client_identd\n",
    "            match.group(3),  # user_id\n",
    "            match.group(4),  # datetime\n",
    "            match.group(5),  # request\n",
    "            int(match.group(6)),  # status\n",
    "            int(match.group(7)),  # size\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Processar o RDD para extrair os campos\n",
    "parsed_logs_rdd = logs_rdd.map(parse_log_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Definir o esquema explicitamente\n",
    "schema = StructType([\n",
    "    StructField(\"ip\", StringType(), True),\n",
    "    StructField(\"client_identd\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"request\", StringType(), True),\n",
    "    StructField(\"status\", IntegerType(), True),\n",
    "    StructField(\"size\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Converter o RDD para DataFrame usando o esquema\n",
    "logs_df = spark.createDataFrame(parsed_logs_rdd, schema=schema)\n",
    "# Converter campo para timestamp\n",
    "logs_df = logs_df.withColumn(\"datetime\", F.to_timestamp(F.col(\"datetime\"), \"dd/MMM/yyyy:HH:mm:ss Z\"))\n",
    "\n",
    "# Mostrar os dados\n",
    "logs_df.show(truncate=True)\n",
    "\n",
    "def write_to_delta_table(df, table_name, path=None):\n",
    "    \"\"\"\n",
    "    Escreve um DataFrame no formato Delta em um caminho especificado e registra como uma tabela Delta.\n",
    "\n",
    "    :param df: DataFrame do PySpark a ser salvo.\n",
    "    :param table_name: Nome da tabela Delta a ser criada.\n",
    "    :param path: Caminho onde os dados Delta serão armazenados (opcional).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if path:\n",
    "            # Escrever o DataFrame no formato Delta em um caminho específico\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", path).saveAsTable(table_name)\n",
    "            print(f\"Tabela Delta '{table_name}' salva com sucesso no caminho '{path}'.\")\n",
    "        else:\n",
    "            # Escrever o DataFrame no formato Delta sem especificar o caminho\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "            print(f\"Tabela Delta '{table_name}' salva com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar a tabela Delta '{table_name}': {e}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "delta_table_name = \"access_log\"\n",
    "#delta_path = \"/mnt/delta-tables/access_log\"  # Substitua pelo caminho desejado\n",
    "\n",
    "# Chamar a função para salvar o DataFrame como Delta\n",
    "write_to_delta_table(logs_df, delta_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 1 executado...\n",
      "+--------------+------+\n",
      "|ip            |count |\n",
      "+--------------+------+\n",
      "|10.216.113.172|109523|\n",
      "|10.173.141.213|45836 |\n",
      "|10.220.112.1  |43910 |\n",
      "|10.41.69.177  |33991 |\n",
      "|10.169.128.121|22516 |\n",
      "|10.203.77.198 |18754 |\n",
      "|10.96.173.111 |17122 |\n",
      "|10.53.149.243 |16706 |\n",
      "|10.31.77.18   |16692 |\n",
      "|10.118.250.30 |15779 |\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def get_top_n_ips(df: DataFrame, n: int = 10) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retorna os top N IPs com mais acessos em um DataFrame.\n",
    "\n",
    "    :param df: DataFrame do PySpark contendo os dados de log.\n",
    "    :param n: Número de IPs a serem retornados (padrão é 10).\n",
    "    :return: DataFrame contendo os top N IPs com mais acessos.\n",
    "    \"\"\"\n",
    "    # Agrupar por IP e contar os acessos\n",
    "    ip_access_count = df.groupBy(\"ip\").count()\n",
    "\n",
    "    # Ordenar em ordem decrescente pelo número de acessos e selecionar os N primeiros\n",
    "    top_n_ips = ip_access_count.orderBy(F.desc(\"count\")).limit(n)\n",
    "\n",
    "    print(f\"Exercício 1 executado...\")\n",
    "\n",
    "    return top_n_ips\n",
    "\n",
    "# Exemplo de uso\n",
    "top_10_ips = get_top_n_ips(logs_df, n=10)\n",
    "top_10_ips.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebc7b76-31f3-4c42-b4de-7b1f0017970b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----+\n",
      "|endpoint                              |count|\n",
      "+--------------------------------------+-----+\n",
      "|/                                     |98793|\n",
      "|/release-schedule/                    |25920|\n",
      "|/search/                              |22985|\n",
      "|/release-schedule                     |18926|\n",
      "|/release-schedule/?p=1&r=&l=&o=&rpp=10|8410 |\n",
      "|/news/                                |7488 |\n",
      "+--------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "def get_top_n_endpoints(df: DataFrame, n: int = 6) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retorna os top N endpoints mais acessados, excluindo aqueles que representam arquivos comuns.\n",
    "    \n",
    "    :param df: DataFrame PySpark contendo os dados de log.\n",
    "    :param n: Número de endpoints a serem retornados (padrão é 6).\n",
    "    :return: DataFrame contendo os top N endpoints mais acessados.\n",
    "    \"\"\"\n",
    "    # Extrair o endpoint do campo \"request\"\n",
    "    df = df.withColumn(\"endpoint\", F.split(F.col(\"request\"), \" \").getItem(1))\n",
    "    \n",
    "    # Filtrar endpoints que não representam arquivos (excluindo extensões comuns)\n",
    "    filtered_df = df.filter(~F.lower(F.col(\"endpoint\")).rlike(\n",
    "        r'\\.(php|css|js|png|jpg|jpeg|gif|ico|svg|woff|ttf|eot|otf|map|json|xml|txt|zip|gz|tar|rar|7z)$'\n",
    "    ))\n",
    "    \n",
    "    # Agrupar por endpoint e contar os acessos\n",
    "    endpoint_access_count = filtered_df.groupBy(\"endpoint\").count()\n",
    "    \n",
    "    # Ordenar em ordem decrescente pelo número de acessos e selecionar os N primeiros\n",
    "    top_n_endpoints = endpoint_access_count.orderBy(F.desc(\"count\")).limit(n)\n",
    "    \n",
    "    print(f\"Exercício 2 executado...\")\n",
    "\n",
    "    return top_n_endpoints\n",
    "\n",
    "# Exemplo de uso\n",
    "top_6_endpoints = get_top_n_endpoints(logs_df, n=6)\n",
    "top_6_endpoints.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeae19ae-c918-4c88-aabc-5a8665b9227f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Client IPs distintos: 330322\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "def count_distinct_ips(df: DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Conta a quantidade de Client IPs distintos em um DataFrame PySpark.\n",
    "\n",
    "    :param df: DataFrame do PySpark contendo os dados de log.\n",
    "    :return: Quantidade de Client IPs distintos.\n",
    "    \"\"\"\n",
    "    # Contar a quantidade de Client IPs distintos\n",
    "    distinct_ips_count = df.select(\"ip\").distinct().count()\n",
    "    return print(f\"Quantidade de Client IPs distintos: {distinct_ips_count}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "distinct_ips_count = count_distinct_ips(logs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9f4eec-0b8c-4340-9a09-860209a00aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de dias distintos representados no arquivo: 791\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def count_distinct_days(df: DataFrame, datetime_col: str) -> int:\n",
    "    \"\"\"\n",
    "    Conta a quantidade de dias distintos representados em um DataFrame PySpark.\n",
    "\n",
    "    :param df: DataFrame PySpark contendo os dados de log.\n",
    "    :param datetime_col: Nome da coluna que contém os valores de data e hora.\n",
    "    :return: Quantidade de dias distintos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrair apenas a data (sem o horário)\n",
    "    df = df.withColumn(\"date\", F.to_date(F.col(datetime_col)))\n",
    "    \n",
    "    # Contar os dias distintos\n",
    "    distinct_days_count = df.select(\"date\").distinct().count()\n",
    "    \n",
    "    return print(f\"Quantidade de dias distintos representados no arquivo: {distinct_days_count}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "distinct_days_count = count_distinct_days(logs_df, \"datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9594688f-202c-4fd8-a0d2-a10c6eace558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume total de dados retornado: 805219137550 bytes\n",
      "Maior volume de dados em uma única resposta: 80215074 bytes\n",
      "Menor volume de dados em uma única resposta: 1 bytes\n",
      "Volume médio de dados retornado: 195016.05 bytes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def analyze_response_volumes(df: DataFrame, datetime_col: str, size_col: str) -> list:\n",
    "    \"\"\"\n",
    "    Realiza a análise de volumes de dados retornados em um DataFrame do PySpark.\n",
    "\n",
    "    :param df: DataFrame do PySpark contendo os dados de log.\n",
    "    :param datetime_col: Nome da coluna que contém os valores de data e hora.\n",
    "    :param size_col: Nome da coluna que contém os tamanhos das respostas.\n",
    "    :return: Uma lista de strings formatadas com os resultados da análise.\n",
    "    \"\"\"\n",
    "    # Converter a coluna datetime para o formato de timestamp\n",
    "    df = df.withColumn(datetime_col, F.to_timestamp(F.col(datetime_col), \"dd/MMM/yyyy:HH:mm:ss Z\"))\n",
    "\n",
    "    # Realizar a análise\n",
    "    results = df.agg(\n",
    "        F.sum(size_col).alias(\"total_volume\"),\n",
    "        F.max(size_col).alias(\"max_volume\"),\n",
    "        F.min(size_col).alias(\"min_volume\"),\n",
    "        F.avg(size_col).alias(\"avg_volume\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    ## Retornar os resultados como um dicionário\n",
    "    #return {\n",
    "    #    \"total_volume\": results[\"total_volume\"],\n",
    "    #    \"max_volume\": results[\"max_volume\"],\n",
    "    #    \"min_volume\": results[\"min_volume\"],\n",
    "    #    \"avg_volume\": results[\"avg_volume\"]\n",
    "    #}\n",
    "\n",
    "    # Retornar os resultados como uma lista de strings formatadas\n",
    "    return [\n",
    "        f\"Volume total de dados retornado: {results['total_volume']} bytes\",\n",
    "        f\"Maior volume de dados em uma única resposta: {results['max_volume']} bytes\",\n",
    "        f\"Menor volume de dados em uma única resposta: {results['min_volume']} bytes\",\n",
    "        f\"Volume médio de dados retornado: {results['avg_volume']:.2f} bytes\"\n",
    "    ]\n",
    "\n",
    "# Exemplo de uso\n",
    "analysis_results = analyze_response_volumes(logs_df, \"datetime\", \"size\")\n",
    "\n",
    "# Exibir os resultados\n",
    "for result in analysis_results:\n",
    "    print(result)\n",
    "\n",
    "\n",
    "## Exibir os resultados\n",
    "#print(f\"Volume total de dados retornado: {analysis_results['total_volume']} bytes\")\n",
    "#print(f\"Maior volume de dados em uma única resposta: {analysis_results['max_volume']} bytes\")\n",
    "#print(f\"Menor volume de dados em uma única resposta: {analysis_results['min_volume']} bytes\")\n",
    "#print(f\"Volume médio de dados retornado: {analysis_results['avg_volume']:.2f} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd5f5b8-a383-4dfa-812b-5daa1ae64a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|day_of_week|count|\n",
      "+-----------+-----+\n",
      "|     Friday|15070|\n",
      "|  Wednesday|12630|\n",
      "|     Monday|12159|\n",
      "|   Thursday|11760|\n",
      "|    Tuesday|11480|\n",
      "|   Saturday|11039|\n",
      "|     Sunday|10523|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "def analyze_http_client_errors_by_day(df: DataFrame, datetime_col: str, status_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Analisa os erros do tipo \"HTTP Client Error\" (status 400-499) em um DataFrame do PySpark\n",
    "    e retorna o número de erros agrupados por dia da semana, ordenados em ordem decrescente.\n",
    "\n",
    "    :param df: DataFrame PySpark contendo os dados de log.\n",
    "    :param datetime_col: Nome da coluna que contém os valores de data e hora.\n",
    "    :param status_col: Nome da coluna que contém os códigos de status HTTP.\n",
    "    :return: DataFrame com os dias da semana e o número de erros, ordenados em ordem decrescente.\n",
    "    \"\"\"\n",
    "    # Adicionar uma coluna para o dia da semana\n",
    "    df = df.withColumn(\"day_of_week\", F.date_format(F.col(datetime_col), \"EEEE\"))\n",
    "\n",
    "    # Filtrar apenas os erros do tipo \"HTTP Client Error\" (status 400-499)\n",
    "    client_errors_df = df.filter((F.col(status_col) >= 400) & (F.col(status_col) < 500))\n",
    "\n",
    "    # Contar o número de erros por dia da semana\n",
    "    errors_by_day = client_errors_df.groupBy(\"day_of_week\").count()\n",
    "\n",
    "    # Ordenar os resultados pelo número de erros em ordem decrescente\n",
    "    errors_by_day_sorted = errors_by_day.orderBy(F.desc(\"count\"))\n",
    "\n",
    "    return errors_by_day_sorted\n",
    "\n",
    "# Exemplo de uso\n",
    "errors_by_day_sorted = analyze_http_client_errors_by_day(logs_df, \"datetime\", \"status\")\n",
    "\n",
    "# Mostrar os resultados\n",
    "errors_by_day_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e32b99-5833-4c09-a309-4a1eafd7e3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|            ip|client_identd|user_id|           datetime|             request|status| size|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|      GET / HTTP/1.1|   403|  202|\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|GET /favicon.ico ...|   404|  209|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|      GET / HTTP/1.1|   200| 9157|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/lo...|   200|10469|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/r...|   200| 1014|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/9...|   200| 6206|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/t...|   200|15779|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/th...|   200| 4492|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/li...|   200|25960|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/s...|   200|  168|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5604|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200|10556|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 9925|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/c...|   200|  979|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/h...|   200| 3892|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5397|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/l...|   200| 2767|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5766|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/h...|   200|68831|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:37|GET /assets/img/d...|   200| 5766|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Criar uma SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Table Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Nome da tabela Delta registrada no metastore\n",
    "delta_table_name = \"access_log\"  # Substitua pelo nome da sua tabela Delta\n",
    "\n",
    "# Ler a tabela Delta pelo nome\n",
    "logs_df = spark.table(delta_table_name)\n",
    "\n",
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01ec6a8-ceb1-4e97-9b15-6169f8a813cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ip</th><th>client_identd</th><th>user_id</th><th>datetime</th><th>request</th><th>status</th><th>size</th></tr></thead><tbody><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T21:58:59Z</td><td>GET / HTTP/1.1</td><td>403</td><td>202</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T21:58:59Z</td><td>GET /favicon.ico HTTP/1.1</td><td>404</td><td>209</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET / HTTP/1.1</td><td>200</td><td>9157</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET /assets/js/lowpro.js HTTP/1.1</td><td>200</td><td>10469</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET /assets/css/reset.css HTTP/1.1</td><td>200</td><td>1014</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET /assets/css/960.css HTTP/1.1</td><td>200</td><td>6206</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET /assets/css/the-associates.css HTTP/1.1</td><td>200</td><td>15779</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET /assets/js/the-associates.js HTTP/1.1</td><td>200</td><td>4492</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:35Z</td><td>GET /assets/js/lightbox.js HTTP/1.1</td><td>200</td><td>25960</td></tr><tr><td>10.223.157.186</td><td>-</td><td>-</td><td>2009-07-15T22:50:36Z</td><td>GET /assets/img/search-button.gif HTTP/1.1</td><td>200</td><td>168</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T21:58:59Z",
         "GET / HTTP/1.1",
         403,
         202
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T21:58:59Z",
         "GET /favicon.ico HTTP/1.1",
         404,
         209
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET / HTTP/1.1",
         200,
         9157
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET /assets/js/lowpro.js HTTP/1.1",
         200,
         10469
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET /assets/css/reset.css HTTP/1.1",
         200,
         1014
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET /assets/css/960.css HTTP/1.1",
         200,
         6206
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET /assets/css/the-associates.css HTTP/1.1",
         200,
         15779
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET /assets/js/the-associates.js HTTP/1.1",
         200,
         4492
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:35Z",
         "GET /assets/js/lightbox.js HTTP/1.1",
         200,
         25960
        ],
        [
         "10.223.157.186",
         "-",
         "-",
         "2009-07-15T22:50:36Z",
         "GET /assets/img/search-button.gif HTTP/1.1",
         200,
         168
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ip",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_identd",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "request",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from access_log limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff9d210-46ff-4a86-bbc0-11be05d2615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|            ip|client_identd|user_id|           datetime|             request|status| size|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|      GET / HTTP/1.1|   403|  202|\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|GET /favicon.ico ...|   404|  209|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|      GET / HTTP/1.1|   200| 9157|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/lo...|   200|10469|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/r...|   200| 1014|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/9...|   200| 6206|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/t...|   200|15779|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/th...|   200| 4492|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/li...|   200|25960|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/s...|   200|  168|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma SparkSession\n",
    "spark = SparkSession.builder.appName(\"Consultar Tabelas\").getOrCreate()\n",
    "\n",
    "\n",
    "# Consultar dados de uma tabela específica\n",
    "dados = spark.sql(\"SELECT * FROM access_log LIMIT 10\")\n",
    "dados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a363b8-eead-4249-9331-f5a929e17ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|            ip|client_identd|user_id|           datetime|             request|status| size|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|      GET / HTTP/1.1|   403|  202|\n",
      "|10.223.157.186|            -|      -|2009-07-15 21:58:59|GET /favicon.ico ...|   404|  209|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|      GET / HTTP/1.1|   200| 9157|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/lo...|   200|10469|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/r...|   200| 1014|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/9...|   200| 6206|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/css/t...|   200|15779|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/th...|   200| 4492|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:35|GET /assets/js/li...|   200|25960|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/s...|   200|  168|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5604|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200|10556|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 9925|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/c...|   200|  979|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/h...|   200| 3892|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5397|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/l...|   200| 2767|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/d...|   200| 5766|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:36|GET /assets/img/h...|   200|68831|\n",
      "|10.223.157.186|            -|      -|2009-07-15 22:50:37|GET /assets/img/d...|   200| 5766|\n",
      "+--------------+-------------+-------+-------------------+--------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregar a tabela diretamente\n",
    "tabela_df = spark.table(\"access_log\")\n",
    "\n",
    "# Mostrar os primeiros registros\n",
    "tabela_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4453449408998945,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Teste_Santander",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
